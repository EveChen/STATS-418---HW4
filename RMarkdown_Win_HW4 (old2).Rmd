---
title: "STATS 418 Homework 4"
author: "Yuan Yi Chen (Eve)"
date: "2017/6/3"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r, include = F}
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")
```
#**Agenda**

###**A. Introduction**
###**B. Brief Summarization**
###**C. Take a Look of Our Data**
###**D. Statistical Models**
###**E. Models, AUC and ROC**
  + #####All Features Included
  + #####Limited Features Included

###**F. Conclusion**

  

***

###**A. Introduction**

####This data is provided a Portuguese banking institution which conducted a marketing campaigns to promote their product. Our goal is to predict if the client will subscribe a term deposit (variable y). We will run several models including neural network, random search (GBM), ensemble, random forest, GBM and logistic regression models. We hope to know:
  * The trade off between training time and accuracy.
  * Will feature selection increases the accuracy?


####The general steps are
  * Manipulate our data set (data wrangling & graphs)
  * Apply different models to the clean data set (with all features)
  * Make prediction and view the accuracy and ROC curve
  * Do feature selection (variable importance)
  * Rebuild models based on limited features
  * Make prediction, AUC and ROC curve (with limited features)

####In order to let readers view the explicit results, I would only include 6 models in this report. That is, regarding to neural network models, only the model with highest AUC will be included in this report. Please refer to the other file __ if you want to view the whole results of totally 26 models.

***

###**B. Brief Summarization** 


#####Here are some pros and cons that I saw from the textbook and Quora
Random Forest Model |          
----|-----------------------------------------------------------
**Pros**|
    | 1. Fast to train our training data
    | 2. High accuracy
    | 3. Automatically Generalized our data within the process
**Cons**| 
    | 1. Inconsistent - because it's classification method
    | 2. Hard to interpret the results
    | 3. Slow to predict our testing data
    | 4. Need high computability, which may be expensive
    
***

###**C. Take a Look of Our Data**

```{r, include = F}
#Remove Objects
rm(list=ls())

#Clear Memory
gc(reset=TRUE)

#diR
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")

#Packages
library(readr)   #Use to read data
library(glmnet)  #Use to apply logistic regression
library(ROCR)    #Use to calcuate AUC
library(h2o)     #Use to run logistic regression and random forest 
library(xgboost) #Use to run random forest model
```

#####We totally have 45211 observations, 15 input variables and 1 output variables (y). The description of features are list as following:


Feature |  Type | Description        
----|-----------|------------------------------------------------
job|Categorical |Type of job, categorical: 'admin.','blue-collar','entrepreneur',etc...
marital| Categorical |Marital status, categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed
education| Categorical | "unknown","secondary","primary","tertiary"
balance| Numeric | Average yearly balance, in euros 
age| Numeric | age
housing| Binary |Has housing loan? (binary: "Y","N")
loan| Binary | Has personal loan? (binary: "Y","N")
contact| Categorical | Contact communication type, categorical: "unknown","telephone","cellular"
day| Numeric |Last contact day of the month
month | Categorical |Last contact month of year, categorical: "jan", "feb", "mar", ..., "nov", "dec"
duration |Numeric |Last contact duration, in seconds
campaign |Numeric |Number of contacts performed during this campaign and for this client
pdays |Numeric |Number of days that passed by after the client was last contacted from a previous campaign
previous |Numeric |Number of contacts performed before this campaign and for this client
poutcome|Categorical |Outcome of the previous marketing campaign, categorical: "unknown","other","failure","success"
**y (output)** |Binary |Has the client subscribed a term deposit? (binary: "Y","N")


```{r, include = F}
dat <- read.csv("test.csv")
```


```{r}
head(dat, 3)
```




###**D. Statistical Models**

###**E. Models, AUC and ROC**
####*Section 1: With all the features

####**1. Neural Network**
```{r, include = F}
library(h2o)
```
```{r, include = F}
h2o.no_progress()
```

```{r, include = F}
h2o.init(nthreads=-1)
```

```{r}
#Split data sets: train, validation and test
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
Xnames <- setdiff(names(dx_train),"y")
```

```{r}
#Neural Network Model (the Best, highest AUC)
system.time({
  md_nn <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid,
            activation = "Rectifier", hidden = c(50,50), 
            adaptive_rate = FALSE, rate = 0.01, rate_annealing = 1e-04, 
            momentum_start = 0.5, momentum_ramp = 1e5, momentum_stable = 0.99,
            epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```
```{r}
#AUC
h2o.performance(md_nn, dx_test)@metrics$AUC
```




####**2. Random Search (GBM)**


####**3. Ensemble (random forest, GBM and neural network)**


####**4. GBM**
```{r}
#Use different data set
dnovalid <- h2o.importFile("test.csv")
dnovalid_split <- h2o.splitFrame(dnovalid, ratios = 0.7, seed = 123)
dnovalid_train <- dnovalid_split[[1]]
dnovalid_test <- dnovalid_split[[2]]
Xnames_novalid <- setdiff(names(dnovalid_train),"y")
```


***


```{r}
#GBM model
system.time({
  md_gbm <- h2o.gbm(x = Xnames_novalid, y = "y", training_frame = dnovalid_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1, nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

***


####**5. Random Forest (Do features selection)**
```{r}
system.time({
  md_rf <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```
```{r}
h2o.auc(h2o.performance(md_rf, dx_test))
```

***

####**6. Logistic Regression**
```{r}
system.time({
  md_logistic <- h2o.glm(x = Xnames_novalid, y = "y", training_frame = dnovalid_train, 
                family = "binomial", 
                alpha = 1, lambda = 0,
                seed = 123,
                nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```
```{r}
h2o.auc(h2o.performance(md_logistic, dnovalid_test))
```



####*Section 2: With limited the features
####**1. Neural Network**


####**2. Random Search (GBM)**


####**3. Ensemble (random forest, GBM and neural network)**


####**4. GBM**


####**5. Random Forest**


####**6. Logistic Regression**








***

###B. Apply models
  * Using Neural Networks
  * Using random search (GBM algorithm)
  * Using ensemble
  * Using Logistic Regression
  * Using random forest
  * Using GBM model

***

#####1. Using Neural Networks
  * Layers (100, 100) with early stopping
  * More layers (100, 100, 100, 100) with early stopping
  * 

#####(a.) Layers (100, 100) with early stopping

Step1: Initiate h2o package
```{r, message = F}
library(h2o)
```
```{r}
h2o.no_progress()
```

```{r, message = F}
h2o.init(nthreads=-1)
```

Step2: Load data & split it into dx_train, dx_valid and dx_test data sets
```{r}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
```
Step3: Build model1
```{r}
Xnames <- names(dx_train)[which(names(dx_train)!="y")]
system.time({
  md1 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid,hidden = c(100,100), epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step4: Make prediction & AUC
```{r}
auc1 <- h2o.performance(md1, dx_test)@metrics$AUC
auc1
```

#####(b.) Layers (100, 100, 100, 100) with early stopping

Step1: Build model1
```{r}
system.time({
  md2 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid,
                         activation = "Rectifier", hidden = c(100,100,100,100), 
                         epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc2 <- h2o.performance(md2, dx_test)@metrics$AUC
auc2
```


#####(c.) Layers (100, 100) with early stopping & dropout rate

Step1: Build model1
```{r}
system.time({
  md3 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), input_dropout_ratio = 0.2, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc3 <- h2o.performance(md3, dx_test)@metrics$AUC
auc3
```

#####(d.) Layers (100, 100) with early stopping & l1 & l2

Step1: Build model1
```{r}
system.time({
  md4 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), l1 = 1e-5, l2 = 1e-5, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc4 <- h2o.performance(md4, dx_test)@metrics$AUC
auc4
```

#####(e.) Layers (100, 100) with early stopping & rho & epsilon

Step1: Build model1
```{r}
system.time({
  md5 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), 
                         rho = 0.95, epsilon = 1e-06, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc5 <- h2o.performance(md5, dx_test)@metrics$AUC
auc5
```

#####(f.) Layers (100, 100) with early stopping & rate & momentum

Step1: Build model1
```{r}
system.time({
  md6 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), adaptive_rate = FALSE, rate = 0.01, rate_annealing = 1e-05, momentum_start = 0.5, momentum_ramp = 1e4, momentum_stable = 0.9, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc6 <- h2o.performance(md6, dx_test)@metrics$AUC
auc6 
```




***

#####2. Random Search (with algorithm "GBM")

Step1: Split data into train, validation and test data sets
```{r, message = F}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
Xnames <- names(dx_train)[which(names(dx_train)!="y")]
```

Step2: Create list of hyperparameters

```{r}
hyper_params <- list( ntrees = 10000,  ## early stopping
                      max_depth = 5:15, 
                      min_rows = c(1,3,10,30,100),
                      learn_rate = c(0.01,0.03,0.1),  
                      learn_rate_annealing = c(0.99,0.995,1,1),
                      sample_rate = c(0.4,0.7,1,1),
                      col_sample_rate = c(0.7,1,1),
                      nbins = c(30,100,300),
                      nbins_cats = c(64,256,1024)
)
```

Step3: Create list of search criteria

```{r}
search_criteria <- list( strategy = "RandomDiscrete",
                         max_runtime_secs = 10*3600,
                         max_models = 100
)
```

Step4: Apply random search with GBM model 
```{r}
system.time({
  md7 <- h2o.grid(algorithm = "gbm", grid_id = "grd",
                  x = Xnames, y = "y", training_frame = dx_train,
                  validation_frame = dx_valid,
                  hyper_params = hyper_params,
                  search_criteria = search_criteria,
                  stopping_metric = "AUC", stopping_tolerance = 1e-3, stopping_rounds = 2,
                  seed = 123)
})
```


Step5: Make prediction and calculate AUC
```{r}
#To sort
md7_sort <- h2o.getGrid(grid_id = "grd", sort_by = "auc", decreasing = TRUE)
md7_sort
```

```{r}
#To see the best
md7_best <- h2o.getModel(md7_sort@model_ids[[1]])
summary(md7_best)
```

```{r}
#AUC 
auc7 <- h2o.auc(h2o.performance(md7_best, dx_test))
auc7
```



***


####3. Using Ensemble

Step1: split our data set 
```{r, message = F}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = 0.7, seed = 123)
dx_train <- dx_split[[1]]
dx_test <- dx_split[[2]]
Xnames <- setdiff(names(dx_train),"y")
```

Step2: Create several models

```{r}
#Logistic Regression model
system.time({
  md_logistic <- h2o.glm(x = Xnames, y = "y", training_frame = dx_train, 
                 family = "binomial", 
                 alpha = 1, lambda = 0,
                 seed = 123,
                 nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#Random Forest model
system.time({
  md_randomforest <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 300, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#GBM model
system.time({
  md_gbm <- h2o.gbm(x = Xnames, y = "y", training_frame = dx_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1,  nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

```{r}
#Neural network model
system.time({
  md_neuralnetwork <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train,  epochs = 5, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE) 
})
```


Step3: Build our ensemble model with the above four models 
```{r}
system.time({
  md_ensemble <- h2o.stackedEnsemble(x = Xnames, y = "y", training_frame = dx_train,base_models = list(md_logistic@model_id, md_randomforest@model_id, md_gbm@model_id, md_neuralnetwork@model_id))
})
```

Step4: Check AUC for all the models
```{r}
h2o.auc(h2o.performance(md_logistic, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_randomforest, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_gbm, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_neuralnetwork, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_ensemble, dx_test))
``` 

Step5: Get coefficients of our ensemble model
```{r}
auc_ensemble <- h2o.getModel(md_ensemble@model$metalearner$name)@model$coefficients_table
```



***

###C. Comparisons
####Compare random forest with logistic model within different packages

Packages         | AUC   | Processing Time (/sec) 
-----------------|-------|------------------
h2o-rf (100 trees) | 0.9314315 | 9.22
h2o-rf (200 trees) | 0.9316806 | 14.76
h2o-rf (300 trees) | 0.9319108 | 18.99
XGBoost-rf (100 trees) | 0.9018877 | 2.28
XGBoost-rf (200 trees) | 0.8982503 | 4.68
XGBoost-rf (300 trees) | 0.9042339 | 6.95
randomForest (100 trees) | Error rate = 9.23% | 4.61
randomForest (200 trees) | Error rate = 9.15% | 8.78
randomForest (300 trees) | Error rate = 9.08% | 11.95
glmnet-glm (w/ lasso)|0.9037  | 0.23
glmnet-glm (w/o lasso)| 0.875|0.18
h2o-glm (w/lasso)    |0.9041  |1.51
h2o-glm (w/o lasso)  |0.8687  |0.52

* ####We can see once we increase the amount of trees, in most cases, the accuracy may go up. However, it may take a bit longer time to train the models.
* ####If we control our method to random forest, we can clearly see the XGBoost package can efficiently train our model (although it's accuracy is worse that other two packages)
* ####If we compare different methods with different packages, we can also see that, in general, random forest model performs better accuracy than logistic model.  



###This is the end of the weekly report. Thank you for your reading! :)

***

