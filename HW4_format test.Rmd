---
title: "Part E. Models - GBM, Random Forest, Logistic Regression, Neural Network"
author: "Yuan Yi Chen (Eve)"
date: "2017年6月6日"
output: html_document
---

```{r, include = F}
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")
```

```{r, include = F}
#Remove Objects
rm(list=ls())

#Clear Memory
gc(reset=TRUE)

#diR
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")

#Packages
library(readr)   #Use to read data
library(glmnet)  #Use to apply logistic regression
library(ROCR)    #Use to calcuate AUC
library(h2o)     #Use to run logistic regression and random forest 
library(xgboost) #Use to run random forest model
library(randomForest)
```

```{r, include = F}
library(h2o)
```
```{r, include = F}
h2o.no_progress()
```

```{r, include = F}
h2o.init(nthreads=-1)
```

```{r, include=F}
#Data sets with validation
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
Xnames <- setdiff(names(dx_train),"y")
```

```{r, include=FALSE}
#Data set without validation
dnovalid <- h2o.importFile("test.csv")
dnovalid_split <- h2o.splitFrame(dnovalid, ratios = 0.7, seed = 123)
dnovalid_train <- dnovalid_split[[1]]
dnovalid_test <- dnovalid_split[[2]]
Xnames_novalid <- setdiff(names(dnovalid_train),"y")
```

###**E. Models**

####**1. GBM**

```{r}
#GBM model
system.time({
  md_gbm <- h2o.gbm(x = Xnames_novalid, y = "y", training_frame = dnovalid_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1, nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

```{r}
h2o.auc(h2o.performance(md_gbm, dnovalid_test))
```

***


####**2. Random Forest**
```{r}
system.time({
  md_rf <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```
```{r}
h2o.auc(h2o.performance(md_rf, dx_test))
```

***

####**3. Logistic Regression**
```{r}
system.time({
  md_logistic <- h2o.glm(x = Xnames_novalid, y = "y", training_frame = dnovalid_train, 
                family = "binomial", 
                alpha = 1, lambda = 0,
                seed = 123,
                nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```
```{r}
h2o.auc(h2o.performance(md_logistic, dnovalid_test))
```

####**4. Neural Network Model**

```{r}
#Neural Network Model (the Best, highest AUC)
system.time({
  md_nn <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid,
            activation = "Rectifier", hidden = c(50,50), 
            adaptive_rate = FALSE, rate = 0.01, rate_annealing = 1e-04, 
            momentum_start = 0.5, momentum_ramp = 1e5, momentum_stable = 0.99,
            epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```
```{r}
#AUC
h2o.performance(md_nn, dx_test)@metrics$AUC
```
***


####**5. Ensemble (random forest, GBM and neural network)**

####**6. Random Search (GBM)**



