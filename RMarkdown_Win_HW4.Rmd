---
title: "STATS 418 Homework 4"
author: "Yuan Yi Chen (Eve)"
date: "2017/6/3"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r, include = F}
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")
```
#**Agenda**

###**A. Brief introduction of What I learn this week**
  * ####Before this Week: 
    + Know how to use **glm**, **glmnet** and **h20** functions to run logistic regression
    + Know in what circumstance should I apply logistic regression model
  * ####After this Week:
    + Know how to use **randomForest package**, **h20 package** and **XGBoost package** functions to run logistic regression
    + Know in what circumstance should I apply random forest model
  * ####Plan for Next Week:
    + Study Basic Neural Network
    + Apply Neural Network to MNist data set with **h2o package** and **Tensorflow**
    
###**B. Random Forest Codes: Using R, H2O and XGBoost packages**
  * ####R - h2o package
  * ####R - XGBoost package
  * ####R - randomForest package
  

***

###A. Brief introduction of What I learn this week

#####Random forest is a popular method to solve different kinds of machine learning tasks. Due to it's high accuracy, it seems to me that it's more prone to a predictive model instead of a description model. Basically, random forest does two things:
  * Randomly choose different features to build different trees
  * Calculate the average value of all the trees it created previously

#####After we apply random forest model to the test data set, We usually can build up confusion matrix, AUC, ROC curve and variable importance to see how does our model perform. Also, there is no limitation about how many trees that we can build. The only concern is the computability of our desktop. That is, random forest seldom overfits in our training set.(Usually, an overfitting will happen if we have only few features to train.) 

#####The general steps are
  * Manipulate our data set (attention to features which are factors)
  * Train the random forest model
  * Predict a test data set
  * View the accuracy, confusion matrix, variable importance
  * Rebuild the random forest model based on the fourth step
  * Predict our test data set again
  * Check the accuracy, confusion matrix, partial dependence
  * Apply other methods to double check if they all come to the same conclusion
  
***

#####Here are some pros and cons that I saw from the textbook and Quora
Random Forest Model |          
----|-----------------------------------------------------------
**Pros**|
    | 1. Fast to train our training data
    | 2. High accuracy
    | 3. Automatically Generalized our data within the process
**Cons**| 
    | 1. Inconsistent - because it's classification method
    | 2. Hard to interpret the results
    | 3. Slow to predict our testing data
    | 4. Need high computability, which may be expensive
    
***

###B. Set up a working environment

Step1: Setup
```{r, include = F}
#Remove Objects
rm(list=ls())

#Clear Memory
gc(reset=TRUE)

#Set Working Directory
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")
```

Step2: Load Packages
```{r, message = F}
library(readr)   #Use to read data
library(glmnet)  #Use to apply logistic regression
library(ROCR)    #Use to calcuate AUC
library(h2o)     #Use to run logistic regression and random forest 
library(xgboost) #Use to run random forest model
```

Step3: Load data set
This **bank marketing** data set is provided by a Portuguese banking institution. It takes records on every phone calls they make to promote their product - term deposit. Our goal is to predict if a customer will subsribe a product after they make phone calls.

```{r}
dat <- read.csv("test.csv")
str(dat)
```

We totally have 45211 observations, 15 input variables and 1 output variables (y).

```{r}
head(dat, 3)
```



***

###B. Apply models
  * Using Neural Networks
  * Using random search (GBM algorithm)
  * Using ensemble
  * Using Logistic Regression
  * Using random forest
  * Using GBM model

***

#####1. Using Neural Networks
  * Layers (100, 100) with early stopping
  * More layers (100, 100, 100, 100) with early stopping
  * 

#####(a.) Layers (100, 100) with early stopping

Step1: Initiate h2o package
```{r, message = F}
library(h2o)
```
```{r}
h2o.no_progress()
```

```{r, message = F}
h2o.init(nthreads=-1)
```

Step2: Load data & split it into dx_train, dx_valid and dx_test data sets
```{r}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
```
Step3: Build model1
```{r}
Xnames <- names(dx_train)[which(names(dx_train)!="y")]
system.time({
  md1 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid,hidden = c(100,100), epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step4: Make prediction & AUC
```{r}
auc1 <- h2o.performance(md1, dx_test)@metrics$AUC
auc1
```

#####(b.) Layers (100, 100, 100, 100) with early stopping

Step1: Build model1
```{r}
system.time({
  md2 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid,
                         activation = "Rectifier", hidden = c(100,100,100,100), 
                         epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc2 <- h2o.performance(md2, dx_test)@metrics$AUC
auc2
```


#####(c.) Layers (100, 100) with early stopping & dropout rate

Step1: Build model1
```{r}
system.time({
  md3 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), input_dropout_ratio = 0.2, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc3 <- h2o.performance(md3, dx_test)@metrics$AUC
auc3
```

#####(d.) Layers (100, 100) with early stopping & l1 & l2

Step1: Build model1
```{r}
system.time({
  md4 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), l1 = 1e-5, l2 = 1e-5, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc4 <- h2o.performance(md4, dx_test)@metrics$AUC
auc4
```

#####(e.) Layers (100, 100) with early stopping & rho & epsilon

Step1: Build model1
```{r}
system.time({
  md5 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), 
                         rho = 0.95, epsilon = 1e-06, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc5 <- h2o.performance(md5, dx_test)@metrics$AUC
auc5
```

#####(f.) Layers (100, 100) with early stopping & rate & momentum

Step1: Build model1
```{r}
system.time({
  md6 <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, validation_frame = dx_valid, activation = "Rectifier", hidden = c(100,100), adaptive_rate = FALSE, rate = 0.01, rate_annealing = 1e-05, momentum_start = 0.5, momentum_ramp = 1e4, momentum_stable = 0.9, epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
```

Step2: Make prediction & AUC
```{r}
auc6 <- h2o.performance(md6, dx_test)@metrics$AUC
auc6 
```




***

#####2. Random Search (with algorithm "GBM")

Step1: Split data into train, validation and test data sets
```{r, message = F}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
Xnames <- names(dx_train)[which(names(dx_train)!="y")]
```

Step2: Create list of hyperparameters

```{r}
hyper_params <- list( ntrees = 10000,  ## early stopping
                      max_depth = 5:15, 
                      min_rows = c(1,3,10,30,100),
                      learn_rate = c(0.01,0.03,0.1),  
                      learn_rate_annealing = c(0.99,0.995,1,1),
                      sample_rate = c(0.4,0.7,1,1),
                      col_sample_rate = c(0.7,1,1),
                      nbins = c(30,100,300),
                      nbins_cats = c(64,256,1024)
)
```

Step3: Create list of search criteria

```{r}
search_criteria <- list( strategy = "RandomDiscrete",
                         max_runtime_secs = 10*3600,
                         max_models = 100
)
```

Step4: Apply random search with GBM model 
```{r}
system.time({
  md7 <- h2o.grid(algorithm = "gbm", grid_id = "grd",
                  x = Xnames, y = "y", training_frame = dx_train,
                  validation_frame = dx_valid,
                  hyper_params = hyper_params,
                  search_criteria = search_criteria,
                  stopping_metric = "AUC", stopping_tolerance = 1e-3, stopping_rounds = 2,
                  seed = 123)
})
```


Step5: Make prediction and calculate AUC
```{r}
#To sort
md7_sort <- h2o.getGrid(grid_id = "grd", sort_by = "auc", decreasing = TRUE)
md7_sort
```

```{r}
#To see the best
md7_best <- h2o.getModel(md7_sort@model_ids[[1]])
summary(md7_best)
```

```{r}
#AUC 
auc7 <- h2o.auc(h2o.performance(md7_best, dx_test))
auc7
```



***


####3. Using Ensemble

Step1: split our data set 
```{r, message = F}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = 0.7, seed = 123)
dx_train <- dx_split[[1]]
dx_test <- dx_split[[2]]
Xnames <- setdiff(names(dx_train),"y")
```

Step2: Create several models

```{r}
#Logistic Regression model
system.time({
  md_logistic <- h2o.glm(x = Xnames, y = "y", training_frame = dx_train, 
                 family = "binomial", 
                 alpha = 1, lambda = 0,
                 seed = 123,
                 nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#Random Forest model
system.time({
  md_randomforest <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 300, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#GBM model
system.time({
  md_gbm <- h2o.gbm(x = Xnames, y = "y", training_frame = dx_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1,  nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

```{r}
#Neural network model
system.time({
  md_neuralnetwork <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train,  epochs = 5, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE) 
})
```


Step3: Build our ensemble model with the above four models 
```{r}
system.time({
  md_ensemble <- h2o.stackedEnsemble(x = Xnames, y = "y", training_frame = dx_train,base_models = list(md_logistic@model_id, md_randomforest@model_id, md_gbm@model_id, md_neuralnetwork@model_id))
})
```

Step4: Check AUC for all the models
```{r}
h2o.auc(h2o.performance(md_logistic, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_randomforest, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_gbm, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_neuralnetwork, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_ensemble, dx_test))
``` 

Step5: Get coefficients of our ensemble model
```{r}
auc_ensemble <- h2o.getModel(md_ensemble@model$metalearner$name)@model$coefficients_table
```



***

###C. Comparisons
####Compare random forest with logistic model within different packages

Packages         | AUC   | Processing Time (/sec) 
-----------------|-------|------------------
h2o-rf (100 trees) | 0.9314315 | 9.22
h2o-rf (200 trees) | 0.9316806 | 14.76
h2o-rf (300 trees) | 0.9319108 | 18.99
XGBoost-rf (100 trees) | 0.9018877 | 2.28
XGBoost-rf (200 trees) | 0.8982503 | 4.68
XGBoost-rf (300 trees) | 0.9042339 | 6.95
randomForest (100 trees) | Error rate = 9.23% | 4.61
randomForest (200 trees) | Error rate = 9.15% | 8.78
randomForest (300 trees) | Error rate = 9.08% | 11.95
glmnet-glm (w/ lasso)|0.9037  | 0.23
glmnet-glm (w/o lasso)| 0.875|0.18
h2o-glm (w/lasso)    |0.9041  |1.51
h2o-glm (w/o lasso)  |0.8687  |0.52

* ####We can see once we increase the amount of trees, in most cases, the accuracy may go up. However, it may take a bit longer time to train the models.
* ####If we control our method to random forest, we can clearly see the XGBoost package can efficiently train our model (although it's accuracy is worse that other two packages)
* ####If we compare different methods with different packages, we can also see that, in general, random forest model performs better accuracy than logistic model.  



###This is the end of the weekly report. Thank you for your reading! :)

***

