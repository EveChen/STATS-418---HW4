---
title: "STATS 418 Homework 4"
author: "Yuan Yi Chen (Eve)"
date: "2017/6/7"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r, include = F}
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")
```
#**Agenda**

###**A. Introduction**
###**B. Brief Summarization**
###**C. Take a Look of Our Data**
###**D. Models, AUC and ROC**
###**E. Conclusion**

  

***

###**A. Introduction**

####This data is provided a Portuguese banking institution which conducted a marketing campaigns to promote their product. Our goal is to predict if the client will subscribe a term deposit (variable y). We will run several models including neural network, random search (GBM), ensemble, random forest, GBM and logistic regression models. We hope to know:
  * The trade off between training time and accuracy.
  * Will feature selection increases the accuracy?


####The general steps are
  * Manipulate our data set (data wrangling & graphs)
  * Apply different models to the clean data set (with all features)
  * Make prediction and view the accuracy and ROC curve
  * Do feature selection (variable importance)
  * Rebuild models based on limited features
  * Make prediction, AUC and ROC curve (with limited features)

####In order to let readers view the explicit results, I would only include 6 models in this report. That is, regarding to neural network models, only the model with highest AUC will be included in this report. Please refer to the other file __ if you want to view the whole results of totally 26 models.

***

###**B. Brief Summarization** 

#####From the following table, We can clearly see that all the AUCs of models fall between 0.9 and 0.94. This means after we train and validate our data set, the prediction of whether consumers will subscribe a product has a high AUC between 0.9 and 0.94.

Models | AUC | Processing time (Sec)        
----|---------|-----------------------------------------------
**Neural Network (the best model within all NN models)**| 0.9246779 | 467.39
**Random Search (with GBM algorithm)**| 0.9272052  |86.29
**Ensemble (random forest + GBM + NN)**| 0.9317667 | 4.05
**GBM**| 0.9272052 | 86.29
**Random Forest**| 0.931621 | 160.78
**Logistic Regression**| 0.9049404 | 2.78

#####In a tradeoff perspective, we can further think about the tradeoff between accuracy(AUC) and the processing time. For example, although the logistic regression has the smallest processing time, its accuracy falls behind other models. Generally speaking, it's not a bad model because perhaps 2~3% AUC difference may not have great impact to our prediction. However, If it's the bioinformatic data, then it's worth to spend lots of time and CPU/GPC to get the model because even 0.01% increases of AUC can benefit the society.


#####In sum, it seems to me that I would choose the logistic model because of its short processing time. In the marketing world, timing and the feedback from our customers are the most important things. As an analyst in this bank who co-works with the marketing team, I am more prone to send the results to the marketing team for them to modify their marketing strategy.

#####On the other hand, if this bank pursues the accuracy (AUC), I may use random forest model instead. It's because although the ensemble model increase the AUC for around 0.001, random forest has the shortest processing time. Furthermore, if we have enough computational resources, we can increase the amount of trees to have higher accuracy. This may not cause the overfitting.

***

###**C. Take a Look of Our Data**

```{r, include = F}
#Remove Objects
rm(list=ls())

#Clear Memory
gc(reset=TRUE)

#diR
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")

#Packages
library(readr)   #Use to read data
library(glmnet)  #Use to apply logistic regression
library(ROCR)    #Use to calcuate AUC
library(h2o)     #Use to run logistic regression and random forest 
library(xgboost) #Use to run random forest model
library(ggplot2) #Use to draw plots
```

#####We totally have 45211 observations, 15 input variables and 1 output variables (y). The description of features are list as following:


Feature |  Type | Description        
----|-----------|------------------------------------------------
job|Categorical |Type of job, categorical: 'admin.','blue-collar','entrepreneur',etc...
marital| Categorical |Marital status, categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed
education| Categorical | "unknown","secondary","primary","tertiary"
balance| Numeric | Average yearly balance, in euros 
age| Numeric | age
housing| Binary |Has housing loan? (binary: "Y","N")
loan| Binary | Has personal loan? (binary: "Y","N")
contact| Categorical | Contact communication type, categorical: "unknown","telephone","cellular"
day| Numeric |Last contact day of the month
month | Categorical |Last contact month of year, categorical: "jan", "feb", "mar", ..., "nov", "dec"
duration |Numeric |Last contact duration, in seconds
campaign |Numeric |Number of contacts performed during this campaign and for this client
pdays |Numeric |Number of days that passed by after the client was last contacted from a previous campaign
previous |Numeric |Number of contacts performed before this campaign and for this client
poutcome|Categorical |Outcome of the previous marketing campaign, categorical: "unknown","other","failure","success"
**y (output)** |Binary |Has the client subscribed a term deposit? (binary: "Y","N")


```{r, include = F}
data <- read.csv("test.csv")
```


```{r}
head(data, 3)
```





###**D. Models, AUC and ROC**

```{r, include = F}
library(h2o)
```
```{r, include = F}
h2o.no_progress()
```

```{r, include = F}
h2o.init(nthreads=-1)
```

```{r, include=F}
#Data sets with validation
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
Xnames <- setdiff(names(dx_train),"y")
```



####**1. GBM**

```{r}
system.time({
  md_gbm <- h2o.gbm(x = Xnames, y = "y", training_frame = dx_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1, nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

```{r}
h2o.auc(h2o.performance(md_gbm, dx_test))
```

***


####**2. Random Forest**
```{r}
system.time({
  md_rf <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 300, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
h2o.auc(h2o.performance(md_rf, dx_test))
```

***

####**3. Logistic Regression**
```{r}
system.time({
  md_logistic <- h2o.glm(x = Xnames, y = "y", training_frame = dx_train, 
                family = "binomial", 
                alpha = 1, lambda = 0,
                seed = 123,
                nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```
```{r}
h2o.auc(h2o.performance(md_logistic, dx_test))
```

####**4. Neural Network Model**

```{r}
system.time({
  md_nn <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train, activation = "Rectifier", hidden = c(100,100),adaptive_rate = FALSE, rate = 0.01, rate_annealing = 1e-04,momentum_start = 0.5, momentum_ramp = 1e5, momentum_stable = 0.99, epochs = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
  })
```

```{r}
#AUC
h2o.performance(md_nn, dx_test)@metrics$AUC
```
***

####**5. Ensemble (random forest, GBM and neural network)**
```{r}
system.time({
md_ensemble <- h2o.stackedEnsemble(x = Xnames, y = "y", training_frame = dx_train,base_models = list(md_rf@model_id, md_gbm@model_id, md_nn@model_id))
})
```

Check the AUC of Ensemble model
```{r}
h2o.auc(h2o.performance(md_ensemble, dx_test))
```

Coefficients of ensemble
```{r}
h2o.getModel(md_ensemble@model$metalearner$name)@model$coefficients_table
```

####**6. Random Search (GBM)**

Step1: Change Xnames to Xnames_rs because random search uses different function instead of "setdiff"
*Notes: Originally, Xnames is 
Xnames <- setdiff(names(dx_train),"y")*
```{r, message = F}
Xnames_rs <- names(dx_train)[which(names(dx_train)!="y")]
```

Step2: Create list of hyperparameters

```{r}
hyper_params <- list( ntrees = 10000,  ## early stopping
                      max_depth = 5:15, 
                      min_rows = c(1,3,10,30,100),
                      learn_rate = c(0.01,0.03,0.1),  
                      learn_rate_annealing = c(0.99,0.995,1,1),
                      sample_rate = c(0.4,0.7,1,1),
                      col_sample_rate = c(0.7,1,1),
                      nbins = c(30,100,300),
                      nbins_cats = c(64,256,1024)
)
```

Step3: Create list of search criteria

```{r}
search_criteria <- list( strategy = "RandomDiscrete",
                         max_runtime_secs = 10*3600,
                         max_models = 10
)
```

Step4: Apply random search with GBM model 
```{r}
system.time({
  md_rs <- h2o.grid(algorithm = "gbm", grid_id = "grd",
                  x = Xnames_rs, y = "y", training_frame = dx_train,
                  validation_frame = dx_valid,
                  hyper_params = hyper_params,
                  search_criteria = search_criteria,
                  stopping_metric = "AUC", stopping_tolerance = 1e-3, stopping_rounds = 2,
                  seed = 123)
})
```


Step5: Make prediction and calculate AUC
```{r}
#To sort
md_rs <- h2o.getGrid(grid_id = "grd", sort_by = "auc", decreasing = TRUE)
```

```{r}
#To see the best
md_rs_best <- h2o.getModel(md_rs@model_ids[[1]])
summary(md_rs_best)
```

```{r}
#AUC 
auc_rs <- h2o.auc(h2o.performance(md_rs_best, dx_test))
auc_rs
```



***


####3. Using Ensemble

Step1: split our data set 
```{r, message = F}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = 0.7, seed = 123)
dx_train <- dx_split[[1]]
dx_test <- dx_split[[2]]
Xnames <- setdiff(names(dx_train),"y")
```

Step2: Create several models

```{r}
#Logistic Regression model
system.time({
  md_logistic <- h2o.glm(x = Xnames, y = "y", training_frame = dx_train, 
                 family = "binomial", 
                 alpha = 1, lambda = 0,
                 seed = 123,
                 nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#Random Forest model
system.time({
  md_randomforest <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 300, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#GBM model
system.time({
  md_gbm <- h2o.gbm(x = Xnames, y = "y", training_frame = dx_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1,  nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

```{r}
#Neural network model
system.time({
  md_neuralnetwork <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train,  epochs = 5, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE) 
})
```


Step3: Build our ensemble model with the above four models 
```{r}
system.time({
  md_ensemble <- h2o.stackedEnsemble(x = Xnames, y = "y", training_frame = dx_train,base_models = list(md_logistic@model_id, md_randomforest@model_id, md_gbm@model_id, md_neuralnetwork@model_id))
})
```

Step4: Check AUC for all the models
```{r}
h2o.auc(h2o.performance(md_logistic, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_randomforest, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_gbm, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_neuralnetwork, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_ensemble, dx_test))
``` 

Step5: Get coefficients of our ensemble model
```{r}
auc_ensemble <- h2o.getModel(md_ensemble@model$metalearner$name)@model$coefficients_table
```



***

###C. Comparisons
####Compare random forest with logistic model within different packages

Packages         | AUC   | Processing Time (/sec) 
-----------------|-------|------------------
h2o-rf (100 trees) | 0.9314315 | 9.22
h2o-rf (200 trees) | 0.9316806 | 14.76
h2o-rf (300 trees) | 0.9319108 | 18.99
XGBoost-rf (100 trees) | 0.9018877 | 2.28
XGBoost-rf (200 trees) | 0.8982503 | 4.68
XGBoost-rf (300 trees) | 0.9042339 | 6.95
randomForest (100 trees) | Error rate = 9.23% | 4.61
randomForest (200 trees) | Error rate = 9.15% | 8.78
randomForest (300 trees) | Error rate = 9.08% | 11.95
glmnet-glm (w/ lasso)|0.9037  | 0.23
glmnet-glm (w/o lasso)| 0.875|0.18
h2o-glm (w/lasso)    |0.9041  |1.51
h2o-glm (w/o lasso)  |0.8687  |0.52

* ####We can see once we increase the amount of trees, in most cases, the accuracy may go up. However, it may take a bit longer time to train the models.
* ####If we control our method to random forest, we can clearly see the XGBoost package can efficiently train our model (although it's accuracy is worse that other two packages)
* ####If we compare different methods with different packages, we can also see that, in general, random forest model performs better accuracy than logistic model.  



###This is the end of the weekly report. Thank you for your reading! :)

***

