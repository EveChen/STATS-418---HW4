---
title: "HW4-test"
author: "Yuan Yi Chen (Eve)"
date: "2017年6月3日"
output: html_document
---

Step1: Setup
```{r, include = F}
#Remove Objects
rm(list=ls())

#Clear Memory
gc(reset=TRUE)

#Set Working Directory
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/418 Tools of Data Science/STATS 418 - HW4")
```

Step2: Load Packages
```{r, message = F}
library(readr)   #Use to read data
library(glmnet)  #Use to apply logistic regression
library(ROCR)    #Use to calcuate AUC
library(h2o)     #Use to run logistic regression and random forest 
library(xgboost) #Use to run random forest model
```

Step3: Load data set
This **bank marketing** data set is provided by a Portuguese banking institution. It takes records on every phone calls they make to promote their product - term deposit. Our goal is to predict if a customer will subsribe a product after they make phone calls.

```{r}
dat <- read.csv("test.csv")
str(dat)
```

We totally have 45211 observations, 15 input variables and 1 output variables (y).

```{r}
head(dat, 3)
```



***

###B. Apply models
  * Using Neural Networks
  * Using random search (GBM algorithm)
  * Using ensemble
  * Using Logistic Regression
  * Using random forest
  * Using GBM model

***

#####1. Using Neural Networks
  * Layers (100, 100) with early stopping
  * More layers (100, 100, 100, 100) with early stopping
  * 

#####(a.) Layers (100, 100) with early stopping

Step1: Initiate h2o package
```{r, message = F}
library(h2o)
```
```{r}
h2o.no_progress()
```

```{r, message = F}
h2o.init(nthreads=-1)
```

Step2: Load data & split it into dx_train, dx_valid and dx_test data sets
```{r}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = c(0.6,0.2), seed = 123)
dx_train <- dx_split[[1]]
dx_valid <- dx_split[[2]]
dx_test <- dx_split[[3]]
```

####3. Using Ensemble

Step1: split our data set 
```{r, message = F}
dx <- h2o.importFile("test.csv")
dx_split <- h2o.splitFrame(dx, ratios = 0.7, seed = 123)
dx_train <- dx_split[[1]]
dx_test <- dx_split[[2]]
Xnames <- setdiff(names(dx_train),"y")
```

Step2: Create several models

```{r}
#Logistic Regression model
system.time({
  md_logistic <- h2o.glm(x = Xnames, y = "y", training_frame = dx_train, 
                 family = "binomial", 
                 alpha = 1, lambda = 0,
                 seed = 123,
                 nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#Random Forest model
system.time({
  md_randomforest <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 300, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)
})
```

```{r}
#GBM model
system.time({
  md_gbm <- h2o.gbm(x = Xnames, y = "y", training_frame = dx_train, distribution = "bernoulli", ntrees = 200, max_depth = 10, learn_rate = 0.1,  nbins = 100, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE)    
})
```

```{r}
#Neural network model
system.time({
  md_neuralnetwork <- h2o.deeplearning(x = Xnames, y = "y", training_frame = dx_train,  epochs = 5, seed = 123, nfolds = 5, fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE) 
})
```


Step3: Build our ensemble model with the above four models 
```{r}
system.time({
  md_ensemble <- h2o.stackedEnsemble(x = Xnames, y = "y", training_frame = dx_train,base_models = list(md_logistic@model_id, md_randomforest@model_id, md_gbm@model_id, md_neuralnetwork@model_id))
})
```

Step4: Check AUC for all the models
```{r}
h2o.auc(h2o.performance(md_logistic, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_randomforest, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_gbm, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_neuralnetwork, dx_test))
``` 

```{r}
h2o.auc(h2o.performance(md_ensemble, dx_test))
``` 

Step5: Get coefficients of our ensemble model
```{r}
auc_ensemble <- h2o.getModel(md_ensemble@model$metalearner$name)@model$coefficients_table
```

